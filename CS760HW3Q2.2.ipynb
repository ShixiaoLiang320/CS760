{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0c7184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Accuracy: 0.8238, Precision: 0.6535, Recall: 0.8140\n",
      "\n",
      "Fold 2:\n",
      "Accuracy: 0.8530, Precision: 0.6857, Recall: 0.8664\n",
      "\n",
      "Fold 3:\n",
      "Accuracy: 0.8620, Precision: 0.7212, Recall: 0.8380\n",
      "\n",
      "Fold 4:\n",
      "Accuracy: 0.8510, Precision: 0.7164, Recall: 0.8163\n",
      "\n",
      "Fold 5:\n",
      "Accuracy: 0.7750, Precision: 0.6057, Recall: 0.7582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('emails.csv')\n",
    "data = data.drop(columns=['Email No.'])\n",
    "X = data.drop(columns=['Prediction']).values\n",
    "y = data['Prediction'].values\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "#1NN algorithm\n",
    "def one_nn(train_features, train_labels, test_features):\n",
    "    predictions = []\n",
    "    for test_instance in test_features:\n",
    "        distances = [euclidean_distance(test_instance, train_instance) for train_instance in train_features]\n",
    "        nearest_neighbor_index = np.argmin(distances)\n",
    "        predictions.append(train_labels[nearest_neighbor_index])\n",
    "    return predictions\n",
    "\n",
    "#Evaluation functions\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred))\n",
    "    return correct / len(y_true)\n",
    "\n",
    "def precision_score(y_true, y_pred, pos_label=1):\n",
    "    true_positives = sum(y_t == y_p == pos_label for y_t, y_p in zip(y_true, y_pred))\n",
    "    predicted_positives = sum(y_p == pos_label for y_p in y_pred)\n",
    "    return true_positives / (predicted_positives or 1)  # avoid division by zero\n",
    "\n",
    "def recall_score(y_true, y_pred, pos_label=1):\n",
    "    true_positives = sum(y_t == y_p == pos_label for y_t, y_p in zip(y_true, y_pred))\n",
    "    actual_positives = sum(y_t == pos_label for y_t in y_true)\n",
    "    return true_positives / (actual_positives or 1)  # avoid division by zero\n",
    "\n",
    "#5-fold cross-validation\n",
    "folds = [(1, 1000), (1000, 2000), (2000, 3000), (3000, 4000), (4000, 5000)]\n",
    "\n",
    "for fold, (start, end) in enumerate(folds):\n",
    "    test_set = X[start:end]\n",
    "    test_labels = y[start:end]\n",
    "    train_set = np.concatenate((X[:start], X[end:]), axis=0)\n",
    "    train_labels = np.concatenate((y[:start], y[end:]), axis=0)\n",
    "    \n",
    "    predictions = one_nn(train_set, train_labels, test_set)\n",
    "    \n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    precision = precision_score(test_labels, predictions)\n",
    "    recall = recall_score(test_labels, predictions)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b5811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
