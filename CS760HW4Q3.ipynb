{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad6adf76",
   "metadata": {},
   "source": [
    "## 3.1 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "252d619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior probability for English: 0.3333\n",
      "prior probability for Spanish: 0.3333\n",
      "prior probability for Japaness: 0.3333\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "dir_path = '/Users/shixiaoliang/Documents/Fall23/CS760/760HW4/languageID'\n",
    "alpha = 0.5\n",
    "K = 3\n",
    "\n",
    "def read_files(language):\n",
    "    total_chars = 0\n",
    "    for i in range(10):\n",
    "        with open(os.path.join(dir_path, f'{language}{i}.txt'), 'r', encoding='utf-8') as f:\n",
    "            content = f.read().lower()\n",
    "            for char in content:\n",
    "                if 'a' <= char <= 'z' or char == ' ':\n",
    "                    total_chars += 1\n",
    "    return total_chars\n",
    "\n",
    "english_chars = read_files('e')\n",
    "spanish_chars = read_files('s')\n",
    "japanese_chars = read_files('j')\n",
    "\n",
    "prior_english = (10 + alpha) / (30 + alpha * K)\n",
    "prior_spanish = (10 + alpha) / (30 + alpha * K)\n",
    "prior_japanese = (10 + alpha) / (30 + alpha * K)\n",
    "\n",
    "print(f\"prior probability for English: {prior_english:.4f}\")\n",
    "print(f\"prior probability for Spanish: {prior_spanish:.4f}\")\n",
    "print(f\"prior probability for Japaness: {prior_japanese:.4f}\")\n",
    "\n",
    "log_prior_e = math.log(prior_english)\n",
    "log_prior_s = math.log(prior_spanish)\n",
    "log_prior_j = math.log(prior_japanese)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed695dc",
   "metadata": {},
   "source": [
    "## 3.2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88c2f336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta for English (θ_e): [0.0601685114819098, 0.011134974392863043, 0.021509995043779945, 0.021972575582355856, 0.1053692383941847, 0.018932760614571286, 0.017478936064761277, 0.047216256401784236, 0.055410540227986124, 0.001420783082768875, 0.0037336857756484387, 0.028977366595076822, 0.020518751032545846, 0.057921691723112505, 0.06446390219725756, 0.01675202378985627, 0.0005617049396993227, 0.053824549810011564, 0.06618205848339666, 0.08012555757475633, 0.026664463902197257, 0.009284652238559392, 0.015496448042293078, 0.001156451346439782, 0.013844374690236246, 0.0006277878737815959, 0.1792499586981662]\n"
     ]
    }
   ],
   "source": [
    "def compute_theta(language):\n",
    "    char_counts = {char: 0 for char in \"abcdefghijklmnopqrstuvwxyz \"}\n",
    "    total_chars = 0\n",
    "    \n",
    "\n",
    "    for i in range(10):\n",
    "        with open(os.path.join(dir_path, f'{language}{i}.txt'), 'r', encoding='utf-8') as f:\n",
    "            content = f.read().lower()\n",
    "            for char in content:\n",
    "                if char in char_counts:\n",
    "                    char_counts[char] += 1\n",
    "                    total_chars += 1\n",
    "                    \n",
    "    V = 27\n",
    "    thetas = []\n",
    "    for char in \"abcdefghijklmnopqrstuvwxyz \":\n",
    "        theta = (char_counts[char] + alpha) / (total_chars + alpha * V)\n",
    "        thetas.append(theta)\n",
    "        \n",
    "    return thetas\n",
    "\n",
    "theta_e = compute_theta('e')\n",
    "\n",
    "print(f\"Theta for English (θ_e): {theta_e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa5cde",
   "metadata": {},
   "source": [
    "## 3.3 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22ce01a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta for Japanese (θ_j): [0.1317656102589189, 0.010866906600510151, 0.005485866033054963, 0.01722631818022992, 0.06020475907613823, 0.003878542227191726, 0.014011670568503443, 0.03176211607673224, 0.09703343932352633, 0.0023411020650616725, 0.05740941332681086, 0.001432614696530277, 0.03979873510604843, 0.05671057688947902, 0.09116321324993885, 0.0008735455466648031, 0.00010482546559977637, 0.04280373178657535, 0.0421747789929767, 0.056990111464411755, 0.07061742199238269, 0.0002445927530661449, 0.01974212935462455, 3.4941821866592126e-05, 0.01415143785596981, 0.00772214263251686, 0.12344945665466997]\n",
      "Theta for Spanish (θ_s): [0.10456045141993771, 0.008232863618143134, 0.03752582405722919, 0.039745922111559924, 0.1138108599796491, 0.00860287996053159, 0.0071844839813758445, 0.0045327001942585795, 0.049859702136844375, 0.006629459467793161, 0.0002775122567913416, 0.052943171656748174, 0.02580863988159477, 0.054176559464709693, 0.07249236841293824, 0.02426690512164287, 0.007677839104560451, 0.05929511886774999, 0.06577040485954797, 0.03561407295488884, 0.03370232185254849, 0.00588942678301625, 9.250408559711388e-05, 0.0024976103111220747, 0.007862847275754679, 0.0026826184823163022, 0.16826493170115014]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "theta_j = compute_theta('j')\n",
    "\n",
    "theta_s = compute_theta('s')\n",
    "\n",
    "print(f\"Theta for Japanese (θ_j): {theta_j}\")\n",
    "print(f\"Theta for Spanish (θ_s): {theta_s}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532df227",
   "metadata": {},
   "source": [
    "## 3.4 ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96810b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words vector for e10.txt: [164, 32, 53, 57, 311, 55, 51, 140, 140, 3, 6, 85, 64, 139, 182, 53, 3, 141, 186, 225, 65, 31, 47, 4, 38, 2, 498]\n"
     ]
    }
   ],
   "source": [
    "def compute_bag_of_words(filename):\n",
    "    char_counts = {char: 0 for char in \"abcdefghijklmnopqrstuvwxyz \"}\n",
    "    \n",
    "    with open(os.path.join(dir_path, filename), 'r', encoding='utf-8') as f:\n",
    "        content = f.read().lower()\n",
    "        for char in content:\n",
    "            if char in char_counts:\n",
    "                char_counts[char] += 1\n",
    "    \n",
    "    # Convert dictionary to vector (list)\n",
    "    bow_vector = [char_counts[char] for char in \"abcdefghijklmnopqrstuvwxyz \"]\n",
    "    \n",
    "    return bow_vector\n",
    "\n",
    "dir_path = '/Users/shixiaoliang/Documents/Fall23/CS760/760HW4/languageID'\n",
    "x = compute_bag_of_words('e10.txt')\n",
    "\n",
    "print(f\"Bag-of-words vector for e10.txt: {x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6c51f",
   "metadata": {},
   "source": [
    "## 3.5 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5e0a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood(bow_vector, thetas):\n",
    "    likelihood = 1\n",
    "    for xi, theta in zip(bow_vector, thetas):\n",
    "        likelihood *= theta ** xi\n",
    "    return likelihood\n",
    "\n",
    "# Compute likelihood for each class\n",
    "p_x_given_e = compute_likelihood(x, theta_e)\n",
    "p_x_given_j = compute_likelihood(x, theta_j)\n",
    "p_x_given_s = compute_likelihood(x, theta_s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bbc56d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log P(x | y=e): -7841.865447060635\n",
      "Log P(x | y=j): -8771.433079075032\n",
      "Log P(x | y=s): -8467.282044010557\n"
     ]
    }
   ],
   "source": [
    "def compute_log_likelihood(bow_vector, thetas):\n",
    "    log_likelihood = 0\n",
    "    for xi, theta in zip(bow_vector, thetas):\n",
    "        if xi > 0:\n",
    "            log_likelihood += xi * math.log(theta)\n",
    "    return log_likelihood\n",
    "\n",
    "# Compute log likelihood for each class\n",
    "log_p_x_given_e = compute_log_likelihood(x, theta_e)\n",
    "log_p_x_given_j = compute_log_likelihood(x, theta_j)\n",
    "log_p_x_given_s = compute_log_likelihood(x, theta_s)\n",
    "\n",
    "print(f\"Log P(x | y=e): {log_p_x_given_e}\")\n",
    "print(f\"Log P(x | y=j): {log_p_x_given_j}\")\n",
    "print(f\"Log P(x | y=s): {log_p_x_given_s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6367239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(x | y=e): e^(-7841.865447060635)\n",
      "P(x | y=j): e^(-8771.433079075032)\n",
      "P(x | y=s): e^(-8467.282044010557)\n"
     ]
    }
   ],
   "source": [
    "print(f\"P(x | y=e): e^({log_p_x_given_e})\")\n",
    "print(f\"P(x | y=j): e^({log_p_x_given_j})\")\n",
    "print(f\"P(x | y=s): e^({log_p_x_given_s})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8b4c2",
   "metadata": {},
   "source": [
    "## 3.6 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9815279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=e | x): e^(-7842.964059349303)\n",
      "P(y=j | x): e^(-8772.5316913637)\n",
      "P(y=s | x): e^(-8468.380656299225)\n",
      "Predicted class label of x: e\n"
     ]
    }
   ],
   "source": [
    "# Compute posterior probabilities using Bayes rule\n",
    "def compute_posterior(log_likelihood, log_prior):\n",
    "    return log_likelihood + log_prior\n",
    "\n",
    "log_posterior_e = compute_posterior(log_p_x_given_e, log_prior_e)\n",
    "log_posterior_j = compute_posterior(log_p_x_given_j, log_prior_j)\n",
    "log_posterior_s = compute_posterior(log_p_x_given_s, log_prior_s)\n",
    "\n",
    "print(f\"P(y=e | x): e^({log_posterior_e})\")\n",
    "print(f\"P(y=j | x): e^({log_posterior_j})\")\n",
    "print(f\"P(y=s | x): e^({log_posterior_s})\")\n",
    "\n",
    "# Predict the class label\n",
    "predicted_class = 'e'\n",
    "if log_posterior_j > log_posterior_e and log_posterior_j > log_posterior_s:\n",
    "    predicted_class = 'j'\n",
    "elif log_posterior_s > log_posterior_e and log_posterior_s > log_posterior_j:\n",
    "    predicted_class = 's'\n",
    "\n",
    "print(f\"Predicted class label of x: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296a947",
   "metadata": {},
   "source": [
    "## 3.7 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "082e49c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\t\tTrue Classes\n",
      "\t\tE\tJ\tS\n",
      "Predicted\n",
      "E\t\t10\t0\t0\n",
      "J\t\t0\t10\t0\n",
      "S\t\t0\t0\t10\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_test_data(filenames):\n",
    "    data = []\n",
    "    for fname in filenames:\n",
    "        with open(os.path.join(dir_path, fname), 'r', encoding='utf-8') as file:\n",
    "#        with open(fname, 'r') as file:\n",
    "            text = file.read().lower()\n",
    "            data.append(text)\n",
    "    return data\n",
    "\n",
    "vocabulary = [chr(i) for i in range(97, 123)] + [' '] \n",
    "\n",
    "def predict(text):\n",
    "    # Convert text to bag-of-words vector\n",
    "    x = [text.count(char) for char in vocabulary]\n",
    "\n",
    "    # Compute log likelihood for each class\n",
    "    log_p_x_given_e = compute_log_likelihood(x, theta_e)\n",
    "    log_p_x_given_j = compute_log_likelihood(x, theta_j)\n",
    "    log_p_x_given_s = compute_log_likelihood(x, theta_s)\n",
    "\n",
    "    # Compute posterior for each class\n",
    "    log_posterior_e = compute_posterior(log_p_x_given_e, log_prior_e)\n",
    "    log_posterior_j = compute_posterior(log_p_x_given_j, log_prior_j)\n",
    "    log_posterior_s = compute_posterior(log_p_x_given_s, log_prior_s)\n",
    "\n",
    "    # Determine predicted class label\n",
    "    predicted_class = 'e'\n",
    "    if log_posterior_j > log_posterior_e and log_posterior_j > log_posterior_s:\n",
    "        predicted_class = 'j'\n",
    "    elif log_posterior_s > log_posterior_e and log_posterior_s > log_posterior_j:\n",
    "        predicted_class = 's'\n",
    "    return predicted_class\n",
    "\n",
    "# Initialize confusion matrix\n",
    "confusion_matrix = defaultdict(lambda: defaultdict(int))\n",
    "languages = ['e', 'j', 's']\n",
    "\n",
    "# Test and populate confusion matrix\n",
    "for lang in languages:\n",
    "    test_files = [f\"{lang}{i}.txt\" for i in range(10, 20)]\n",
    "    test_data = load_test_data(test_files)\n",
    "    for text in test_data:\n",
    "        predicted = predict(text)\n",
    "        confusion_matrix[predicted][lang] += 1\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"\\t\\tTrue Classes\")\n",
    "print(\"\\t\\tE\\tJ\\tS\")\n",
    "print(\"Predicted\")\n",
    "for predicted in languages:\n",
    "    print(f\"{predicted.upper()}\\t\", end=\"\")\n",
    "    for true in languages:\n",
    "        print(f\"\\t{confusion_matrix[predicted][true]}\", end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa5519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
